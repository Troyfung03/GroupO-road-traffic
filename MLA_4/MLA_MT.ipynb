{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q4  MLA && PCA - Traffic Density -> Traffic Duration / Traffic Clustering - GET DATA HOURLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTITHREADING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting traffic_20230901_hour_0.csv\n",
      "File traffic_20230901_hour_0.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_1.csv\n",
      "File traffic_20230901_hour_1.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_2.csv\n",
      "File traffic_20230901_hour_2.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_3.csv\n",
      "File traffic_20230901_hour_3.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_4.csv\n",
      "File traffic_20230901_hour_4.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_5.csv\n",
      "File traffic_20230901_hour_5.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_6.csv\n",
      "File traffic_20230901_hour_6.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_7.csv\n",
      "File traffic_20230901_hour_7.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_8.csv\n",
      "File traffic_20230901_hour_8.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_9.csv\n",
      "File traffic_20230901_hour_9.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_10.csv\n",
      "File traffic_20230901_hour_10.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_11.csv\n",
      "File traffic_20230901_hour_11.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_12.csv\n",
      "File traffic_20230901_hour_12.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_13.csv\n",
      "File traffic_20230901_hour_13.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_14.csv\n",
      "File traffic_20230901_hour_14.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_15.csv\n",
      "File traffic_20230901_hour_15.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_16.csv\n",
      "File traffic_20230901_hour_16.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_17.csv\n",
      "File traffic_20230901_hour_17.csv already exists. Skipping this hour.\n",
      "Getting traffic_20230901_hour_18.csv\n",
      "File traffic_20230901_hour_18.csv saved.\n",
      "end of the hour 18\n",
      "Getting traffic_20230901_hour_19.csv\n",
      "File traffic_20230901_hour_19.csv saved.\n",
      "end of the hour 19\n",
      "Getting traffic_20230901_hour_20.csv\n",
      "File traffic_20230901_hour_20.csv saved.\n",
      "end of the hour 20\n",
      "Getting traffic_20230901_hour_21.csv\n",
      "File traffic_20230901_hour_21.csv saved.\n",
      "end of the hour 21\n",
      "Getting traffic_20230901_hour_22.csv\n",
      "File traffic_20230901_hour_22.csv saved.\n",
      "end of the hour 22\n",
      "Getting traffic_20230901_hour_23.csv\n",
      "File traffic_20230901_hour_23.csv saved.\n",
      "end of the hour 23\n",
      "Getting traffic_20230902_hour_0.csv\n",
      "File traffic_20230902_hour_0.csv saved.\n",
      "end of the hour 0\n",
      "Getting traffic_20230902_hour_1.csv\n",
      "File traffic_20230902_hour_1.csv saved.\n",
      "end of the hour 1\n",
      "Getting traffic_20230902_hour_2.csv\n",
      "File traffic_20230902_hour_2.csv saved.\n",
      "end of the hour 2\n",
      "Getting traffic_20230902_hour_3.csv\n",
      "File traffic_20230902_hour_3.csv saved.\n",
      "end of the hour 3\n",
      "Getting traffic_20230902_hour_4.csv\n",
      "File traffic_20230902_hour_4.csv saved.\n",
      "end of the hour 4\n",
      "Getting traffic_20230902_hour_5.csv\n",
      "File traffic_20230902_hour_5.csv saved.\n",
      "end of the hour 5\n",
      "Getting traffic_20230902_hour_6.csv\n",
      "File traffic_20230902_hour_6.csv saved.\n",
      "end of the hour 6\n",
      "Getting traffic_20230902_hour_7.csv\n",
      "File traffic_20230902_hour_7.csv saved.\n",
      "end of the hour 7\n",
      "Getting traffic_20230902_hour_8.csv\n",
      "File traffic_20230902_hour_8.csv saved.\n",
      "end of the hour 8\n",
      "Getting traffic_20230902_hour_9.csv\n",
      "File traffic_20230902_hour_9.csv saved.\n",
      "end of the hour 9\n",
      "Getting traffic_20230902_hour_10.csv\n",
      "File traffic_20230902_hour_10.csv saved.\n",
      "end of the hour 10\n",
      "Getting traffic_20230902_hour_11.csv\n",
      "File traffic_20230902_hour_11.csv saved.\n",
      "end of the hour 11\n",
      "Getting traffic_20230902_hour_12.csv\n",
      "File traffic_20230902_hour_12.csv saved.\n",
      "end of the hour 12\n",
      "Getting traffic_20230902_hour_13.csv\n",
      "File traffic_20230902_hour_13.csv saved.\n",
      "end of the hour 13\n",
      "Getting traffic_20230902_hour_14.csv\n",
      "File traffic_20230902_hour_14.csv saved.\n",
      "end of the hour 14\n",
      "Getting traffic_20230902_hour_15.csv\n",
      "File traffic_20230902_hour_15.csv saved.\n",
      "end of the hour 15\n",
      "Getting traffic_20230902_hour_16.csv\n",
      "File traffic_20230902_hour_16.csv saved.\n",
      "end of the hour 16\n",
      "Getting traffic_20230902_hour_17.csv\n",
      "File traffic_20230902_hour_17.csv saved.\n",
      "end of the hour 17\n",
      "Getting traffic_20230902_hour_18.csv\n",
      "File traffic_20230902_hour_18.csv saved.\n",
      "end of the hour 18\n",
      "Getting traffic_20230902_hour_19.csv\n",
      "File traffic_20230902_hour_19.csv saved.\n",
      "end of the hour 19\n",
      "Getting traffic_20230902_hour_20.csv\n",
      "File traffic_20230902_hour_20.csv saved.\n",
      "end of the hour 20\n",
      "Getting traffic_20230902_hour_21.csv\n",
      "File traffic_20230902_hour_21.csv saved.\n",
      "end of the hour 21\n",
      "Getting traffic_20230902_hour_22.csv\n",
      "File traffic_20230902_hour_22.csv saved.\n",
      "end of the hour 22\n",
      "Getting traffic_20230902_hour_23.csv\n",
      "File traffic_20230902_hour_23.csv saved.\n",
      "end of the hour 23\n",
      "finished fetch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime.strptime('20230901', '%Y%m%d')\n",
    "end_date = datetime.strptime('20230902', '%Y%m%d')\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://api.data.gov.hk/v1/historical-archive/get-file?url=https%3A%2F%2Fresource.data.one.gov.hk%2Ftd%2Ftraffic-detectors%2FrawSpeedVol-all.xml&time='\n",
    "\n",
    "# Function to fetch data for a given minute\n",
    "def fetch_data(minute, current_date, hour, totals):\n",
    "    # Generate the timestamp for the current minute\n",
    "    timestamp = current_date.strftime('%Y%m%d') + '-' + f'{hour:02d}{minute:02d}'\n",
    "\n",
    "    # Fetch the data for the current timestamp\n",
    "    response = requests.get(base_url + timestamp)\n",
    "\n",
    "    # Check the status of the response\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {timestamp}, status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    # Extract the data and add it to the dictionary\n",
    "    for period in root.findall('.//periods/period'):\n",
    "        for detector in period.findall('.//detectors/detector'):\n",
    "            detector_id = detector.find('detector_id').text\n",
    "\n",
    "            for lane in detector.findall('.//lanes/lane'):\n",
    "                lane_id_element = lane.find('lane_id')\n",
    "                if lane_id_element is not None:\n",
    "                    lane_id = lane_id_element.text\n",
    "                else:\n",
    "                    print(\"lane_id element not found\")\n",
    "                    continue\n",
    "\n",
    "                speed_element = lane.find('speed')\n",
    "\n",
    "                occupancy = int(lane.find('occupancy').text)\n",
    "                volume = int(lane.find('volume').text)\n",
    "\n",
    "                # Check if the 'speed' element is present and contains a valid float\n",
    "                if speed_element is not None:\n",
    "                    try:\n",
    "                        speed = float(speed_element.text)\n",
    "                    except ValueError:\n",
    "                        print(f\"Invalid speed: {speed_element.text}\")\n",
    "                        continue\n",
    "\n",
    "                    # Add the speed to the total for the current detector and lane id\n",
    "                    if (detector_id, lane_id) not in totals:\n",
    "                        totals[(detector_id, lane_id)] = [speed, 1, occupancy, volume]\n",
    "                    else:\n",
    "                        totals[(detector_id, lane_id)][0] += speed\n",
    "                        totals[(detector_id, lane_id)][1] += 1\n",
    "                        totals[(detector_id, lane_id)][2] += occupancy\n",
    "                        totals[(detector_id, lane_id)][3] += volume\n",
    "\n",
    "# Loop over the date range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Loop over each hour of the current day\n",
    "    for hour in range(0, 24):\n",
    "        # Save the DataFrame to a CSV file\n",
    "        filename = f\"traffic_{current_date.strftime('%Y%m%d')}_hour_{hour}.csv\"\n",
    "        \n",
    "        print('Getting', filename);\n",
    "\n",
    "        if os.path.isfile(filename):\n",
    "            print(f\"File {filename} already exists. Skipping this hour.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize a dictionary to store the total speed and count for each detector and lane\n",
    "        totals = {}\n",
    "\n",
    "        # Create a ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            # Loop over each minute of the current hour\n",
    "            for minute in range(0, 60):\n",
    "                executor.submit(fetch_data, minute, current_date, hour, totals)\n",
    "\n",
    "        # Calculate the average speed for each detector and lane id and add it to the list\n",
    "        data = [{'detectorID': detector_id, 'laneType': lane_id, 'speed': total_speed / count, 'totalOccupancy': total_occupancy, 'totalVolume': total_volume}\n",
    "                for (detector_id, lane_id), (total_speed, count, total_occupancy, total_volume) in totals.items()]\n",
    "\n",
    "        # Convert the list into a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"File {filename} saved.\")\n",
    "\n",
    "        print(f\"end of the hour {hour}\")\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f\"finished fetch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
